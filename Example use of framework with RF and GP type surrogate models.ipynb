{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import GPyOpt\n",
    "import GPy\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.tri as tri\n",
    "import ternary\n",
    "import pickle\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn import preprocessing\n",
    "import pyDOE\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load materials dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Handler():\n",
    "    def __init__(self, dataset_name, maximization=True):\n",
    "\n",
    "        # Parameters:\n",
    "        # dataset_name (string)\n",
    "        #       name of the dataset to use. Options include ['Crossed barrel', 'Perovskite', 'AgNP', 'P3HT', 'AutoAM']\n",
    "        # maximization (boolean)\n",
    "        #       If true then the objective value will be positive (maximization objective problem, use for P3HT/CNT, Crossed barrel, AutoAM)\n",
    "        #       If false then the objective value will be negative (minimzation objective problem, use for Perovskite, AgNP)\n",
    "\n",
    "        self.dataset = self.load_data(dataset_name)\n",
    "        self.normalized_dataset = self.process_data(maximization)\n",
    "\n",
    "\n",
    "    def load_data(self, dataset_name):\n",
    "\n",
    "        # Parameters:\n",
    "        # dataset_name (string)\n",
    "        #       name of the dataset to use. Options include ['Crossed barrel', 'Perovskite', 'AgNP', 'P3HT', 'AutoAM']\n",
    "        # Outputs:\n",
    "        # dataset (pd.DataFrame)\n",
    "        #       dataset in the form of a pandas dataframe\n",
    "\n",
    "        raw_dataset = pd.read_csv('datasets/' + dataset_name + '_dataset.csv')\n",
    "        dataset = copy.deepcopy(raw_dataset) \n",
    "\n",
    "        return dataset\n",
    "    \n",
    "\n",
    "    def process_data(self, maximization):\n",
    "\n",
    "        # Parameters:\n",
    "        # maximization (boolean)\n",
    "        #       If true then the objective value will be positive (maximization objective problem, use for P3HT/CNT, Crossed barrel, AutoAM)\n",
    "        #       If false then the objective value will be negative (minimzation objective problem, use for Perovskite, AgNP)\n",
    "        # Outputs:\n",
    "        # grouped_dataset (pd.DataFrame)\n",
    "        #       to perform pool-based active learning, need to group the data by unique input feature x value\n",
    "        #       for each unique x in design space, only keep the average of all evaluations there as its objective value\n",
    "\n",
    "        feature_names, objective_name = list(self.dataset.columns)[:-1], list(self.dataset.columns)[-1]\n",
    "\n",
    "        if maximization is True:\n",
    "            self.dataset[objective_name] = self.dataset[objective_name].values\n",
    "        if maximization is False:\n",
    "            self.dataset[objective_name] = -self.dataset[objective_name].values\n",
    "    \n",
    "        processed_dataset = self.dataset.groupby(feature_names)[objective_name].agg(lambda x: x.unique().mean())\n",
    "        processed_dataset = (processed_dataset.to_frame()).reset_index()\n",
    "\n",
    "        return processed_dataset\n",
    "    \n",
    "    \n",
    "    def get_top_n_indices(self, top_percentage):\n",
    "\n",
    "        # Parameters:\n",
    "        # top_percentage (float)\n",
    "        #       Denotes the top_percentage of data samples as being one of the top samples in the dataset\n",
    "        #       Ex. top_percentage = 0.05 will retrieve the indices of the top 5% of data samples\n",
    "        # Outputs:\n",
    "        # top_indices (list)\n",
    "        #       list corresponding to the indices of belonging to data samples in the top_percentage of all samples in the dataset\n",
    "\n",
    "        objective_name = list(self.dataset.columns)[-1]\n",
    "\n",
    "        n_top = int(math.ceil(len(self.normalized_dataset) * top_percentage))\n",
    "        top_indices = list(self.normalized_dataset.sort_values(objective_name).head(n_top).index)\n",
    "\n",
    "        return top_indices\n",
    "    \n",
    "\n",
    "    def X_y_split(self):\n",
    "\n",
    "        # Outputs:\n",
    "        # X, y (np.array), (np.array)\n",
    "        #       X, numpy array containing data from the input features\n",
    "        #       y, numpy array containing data from the output feature \n",
    "\n",
    "        feature_names, objective_name = list(self.dataset.columns)[:-1], list(self.dataset.columns)[-1]\n",
    "\n",
    "        X = self.normalized_dataset[feature_names].values\n",
    "        y = np.array(self.normalized_dataset[objective_name].values)\n",
    "\n",
    "        assert len(self.normalized_dataset) == len(X) == len(y), \"X and y have a different amount of rows\"\n",
    "        return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Surrogate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RF_Surrogate_Model():\n",
    "    def __init__(self, n_est = 100, n_jobs = -1):\n",
    "\n",
    "        # Parameters:\n",
    "        # n_est (integer)\n",
    "        #       the number of decision trees to be used in Random Forests Model\n",
    "        #       Paper specifies 100 is suitable for all 5 datasets\n",
    "        # n_jobs (integer)\n",
    "        #       The number of jobs to run in parallel\n",
    "        #       -1 denotes using all processors\n",
    "\n",
    "        self.model = RandomForestRegressor(n_estimators = n_est, n_jobs = n_jobs)\n",
    "        self.n_est = n_est\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Parameters:\n",
    "        # X (np.array)\n",
    "        #       training data from X to train the model on\n",
    "        # y (np.array)\n",
    "        #       training data from y to train the model on\n",
    "\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # Parameters:\n",
    "        # X (np.array)\n",
    "        #       Data from X for the model to form predictions on\n",
    "        # Outputs:\n",
    "        # mean, std (float), (float)\n",
    "        #       mean of the models predictions\n",
    "        #       std of the models predictions\n",
    "\n",
    "        tree_predictions = []\n",
    "\n",
    "        for j in np.arange(self.n_est):\n",
    "            tree_predictions.append((self.model.estimators_[j].predict(np.array([X]))).tolist())\n",
    "\n",
    "        mean = np.mean(np.array(tree_predictions), axis=0)[0]\n",
    "        std = np.std(np.array(tree_predictions), axis=0)[0]\n",
    "\n",
    "        return mean, std\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "class GP_Surrogate_Model():\n",
    "    def __init__(self, X, kernel_name, ARD_):\n",
    "\n",
    "        # Parameters:\n",
    "        # X (np.array)\n",
    "        #       X dataset, used to initialize different kernels\n",
    "        # kernel_name (str)\n",
    "        #       string denoting which kernel to use. Options include ['Mat52', 'Mat32', 'Mat12', 'RBF']\n",
    "        # ARD_ (boolean)\n",
    "        #       If true, initializes GP model with anisotropic kernels\n",
    "        #       If flase, initializes GP model with isotropic kernels\n",
    "\n",
    "        kernel_ = self.get_kernel(X, kernel_name, ARD_)\n",
    "\n",
    "        self.model = GPyOpt.models.GPModel(\n",
    "            optimize_restarts = 10,\n",
    "            kernel = kernel_,\n",
    "            noise_var = 0.01,\n",
    "            max_iters = 100,\n",
    "            ARD = ARD_,\n",
    "            optimizer = 'bfgs',\n",
    "            verbose = False)\n",
    "\n",
    "\n",
    "    def get_kernel(self, X, kernel_name, ARD_):\n",
    "\n",
    "        # Parameters:\n",
    "        # X (np.array)\n",
    "        #       X dataset, used to initialize different kernels\n",
    "        # kernel_name (str)\n",
    "        #       string denoting which kernel to use. Options include ['Mat52', 'Mat32', 'Mat12', 'RBF']\n",
    "        # ARD_ (boolean)\n",
    "        #       If true, initializes GP model with anisotropic kernels\n",
    "        #       If flase, initializes GP model with isotropic kernels\n",
    "        # Outputs:\n",
    "        # kernel (GPy.kern object)\n",
    "        #       returns kernel corresponding to kernel_name\n",
    "\n",
    "        Bias_k = GPy.kern.Bias(X.shape[1], variance=1.)\n",
    "\n",
    "        Matern52_k = GPy.kern.Matern52(X.shape[1], variance=1., ARD=ARD_) + Bias_k\n",
    "        Matern32_k = GPy.kern.Matern32(X.shape[1], variance=1., ARD=ARD_) + Bias_k\n",
    "        Matern12_k = GPy.kern.Exponential(X.shape[1], variance=1., ARD=ARD_) + Bias_k\n",
    "        RBF_k = GPy.kern.RBF(X.shape[1], variance=1., ARD=ARD_) + Bias_k\n",
    "\n",
    "        kernels = {\"Mat52\" : Matern52_k, \n",
    "                   \"Mat32\" : Matern32_k,\n",
    "                   \"Mat12\" : Matern12_k, \n",
    "                   \"RBF\" : RBF_k}\n",
    "\n",
    "        return kernels[kernel_name]\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Parameters:\n",
    "        # X (np.array)\n",
    "        #       training data from X to train the model on\n",
    "        # y (np.array)\n",
    "        #       training data from y to train the model on\n",
    "\n",
    "        self.model.updateModel(X, y, None, None)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # Parameters:\n",
    "        # X (np.array)\n",
    "        #       Data from X for the model to form predictions on\n",
    "        # Outputs:\n",
    "        # mean, std (float), (float)\n",
    "        #       mean of the models predictions\n",
    "        #       std of the models predictions\n",
    "\n",
    "        mean, var = self.model.predict(X)\n",
    "\n",
    "        return mean, np.sqrt(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BO():\n",
    "    def __init__(self, X, y, top_indices):\n",
    "\n",
    "        # Parameters:\n",
    "        # X (np.array)\n",
    "        #       X dataset of input features\n",
    "        # Y (np.array)\n",
    "        #       y dataset of the output label\n",
    "        # top_indices (list)\n",
    "        #       Corresponds to the indices of the top candidates defined earlier\n",
    "        #       Allows for keeping track of if a top candidate has been found\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.top_indices = top_indices\n",
    "\n",
    "        self.index_collection = []\n",
    "        self.X_collection = []\n",
    "        self.y_collection = []\n",
    "        self.TopCount_collection = []\n",
    "\n",
    "\n",
    "    # expected improvement\n",
    "    def EI(self, X, model, y_best):\n",
    "\n",
    "        # Parameters:\n",
    "        # X (np.array)\n",
    "        #       Data samples from X\n",
    "        # model (RF_Surrogate_Model or GP_Surrogate_Model Object)\n",
    "        #       model object, either GP or RF\n",
    "        # y_best (float)\n",
    "        #       Denotes the maximum value in y of the explored data samples in y\n",
    "        # Outputs:\n",
    "        # (integer)\n",
    "        #       Denotes the next X value to explore/exploit     \n",
    "        \n",
    "        xi = 0 # can also use 0.01\n",
    "        mean, std = model.predict(X)\n",
    "\n",
    "        z = (y_best - mean - xi)/std\n",
    "        return (y_best - mean - xi) * norm.cdf(z) + std * norm.pdf(z)\n",
    "\n",
    "\n",
    "    # lower confidence bound\n",
    "    def LCB(self, X, model, ratio):\n",
    "\n",
    "        # Parameters:\n",
    "        # X (np.array)\n",
    "        #       Data samples from X\n",
    "        # model (RF_Surrogate_Model or GP_Surrogate_Model Object)\n",
    "        #       model object, either GP or RF\n",
    "        # ratio (integer)\n",
    "        #       parameter controls whether the acquisition function should focus one exploration or exploitation\n",
    "        #       higher ratio means more focus on exploration\n",
    "        #       lower ratio means more focus on exploitation\n",
    "        # Outputs:\n",
    "        # (integer)\n",
    "        #       Denotes the next X value to explore/exploit  \n",
    "\n",
    "        mean, std = model.predict(X)\n",
    "        \n",
    "        return - mean + ratio * std\n",
    "\n",
    "\n",
    "    # probability of improvement\n",
    "    def PI(self, X, model, y_best):\n",
    "\n",
    "        # Parameters:\n",
    "        # X (np.array)\n",
    "        #       Data samples from X\n",
    "        # model (RF_Surrogate_Model or GP_Surrogate_Model Object)\n",
    "        #       model object, either GP or RF\n",
    "        # y_best (float)\n",
    "        #       Denotes the maximum value in y of the explored data samples in y\n",
    "        # Outputs:\n",
    "        # (integer)\n",
    "        #       Denotes the next X value to explore/exploit  \n",
    "\n",
    "        xi = 0 # can also use 0.01\n",
    "        mean, std = model.predict(X)\n",
    "        \n",
    "        z = (y_best - mean - xi)/std\n",
    "        return norm.cdf(z)\n",
    "    \n",
    "    def find_next_index(self, indices_to_explore, model, acqusition_function_name, y_best):\n",
    "        next_index = None\n",
    "\n",
    "        #initialize negative number to ensure max_ac gets overwritten in following for loop\n",
    "        max_ac = -10**10\n",
    "\n",
    "        for j in indices_to_explore:\n",
    "            X_j = self.X[j]\n",
    "            y_j = self.y[j]\n",
    "\n",
    "            # select Acquisiton Function for BO\n",
    "            if acqusition_function_name == \"LCB\":\n",
    "                ac_value = self.LCB(X_j, model, 2)\n",
    "\n",
    "            elif acqusition_function_name == \"EI\":\n",
    "                ac_value = self.EI(X_j, model, y_best)\n",
    "\n",
    "            elif acqusition_function_name == \"PI\":\n",
    "                ac_value = self.PI(X_j, model, y_best)\n",
    "\n",
    "            # in BO explore index with largest acquisition function value\n",
    "            if max_ac <= ac_value:\n",
    "                max_ac = ac_value\n",
    "                next_index = j\n",
    "\n",
    "        # return best index to explore/exploit according to acquisition function\n",
    "        return next_index\n",
    "    \n",
    "    \n",
    "    def run_bayesian_optimization(self, \n",
    "                                  n_ensemble = 50, \n",
    "                                  n_initial = 2,\n",
    "                                  surrogate_model_name = \"RF\",\n",
    "                                  acqusition_function_name = \"LCB\",\n",
    "                                  **kwargs):\n",
    "        \n",
    "        # Parameters:\n",
    "        # n_ensemble (integer)\n",
    "        #       The number of ensemble models to run, paper uses 50\n",
    "        # n_initial (integer)\n",
    "        #       The number of initial experiments to run, paper uses 2\n",
    "        # surrogate_model_name (string)\n",
    "        #       The name of the surrogate model to use. Options include  ['RF', 'GP']\n",
    "        # acquisition_function_name\n",
    "        #       The name of the acquisition function to use. Options include  ['LCB', 'EI', 'PI']\n",
    "        # Outputs:\n",
    "        # (npy file)\n",
    "        #       Returns npy file of results from bayesian optimization algorithm\n",
    "        \n",
    "        start_time = time.time()\n",
    "        seed_list = seed_list = [4295, 8508, 326, 3135, 1549, 2528, 1274, 6545, 5971, 6269, 2422, 4287, 9320, 4932, 951, 4304, 1745, 5956, 7620, 4545, 6003, 9885, 5548, 9477, 30, 8992, 7559, 5034, 9071, 6437, 3389, 9816, 8617, 3712, 3626, 1660, 3309, 2427, 9872, 938, 5156, 7409, 7672, 3411, 3559, 9966, 7331, 8273, 8484, 5127, 2260, 6054, 5205, 311, 6056, 9456, 928, 6424, 7438, 8701, 8634, 4002, 6634, 8102, 8503, 1540, 9254, 7972, 7737, 3410, 4052, 8640, 9659, 8093, 7076, 7268, 2046, 7492, 3103, 3034, 7874, 5438, 4297, 291, 5436, 9021, 3711, 7837, 9188, 2036, 8013, 6188, 3734, 187, 1438, 1061, 674, 777, 7231, 7096, 3360, 4278, 5817, 5514, 3442, 6805, 6750, 8548, 9751, 3526, 9969, 8979, 1526, 1551, 2058, 6325, 1237, 5917, 5821, 9946, 5049, 654, 7750, 5149, 3545, 9165, 2837, 5621, 6501, 595, 3181, 1747, 4405, 4480, 4282, 9262, 6219, 3960, 4999, 1495, 6007, 9642, 3902, 3133, 1085, 3278, 1104, 5939, 7153, 971, 8733, 3785, 9056, 2020, 7249, 5021, 3384, 8740, 4593, 7869, 9941, 8813, 3688, 8139, 6436, 3742, 5503, 1587, 4766, 9846, 9117, 7001, 4853, 9346, 4927, 8480, 5298, 4753, 1151, 9768, 5405, 6196, 5721, 3419, 8090, 8166, 7834, 1480, 1150, 9002, 1134, 2237, 3995, 2029, 5336, 7050, 6857, 8794, 1754, 1184, 3558, 658, 6804, 8750, 5088, 1136, 626, 8462, 5203, 3196, 979, 7419, 1162, 5451, 6492, 1562, 8145, 8937, 8764, 4174, 7639, 8902, 7003, 765, 1554, 6135, 1689, 9530, 1398, 2273, 7925, 5948, 1036, 868, 4617, 1203, 7680, 7, 93, 3128, 5694, 6979, 7136, 8084, 5770, 9301, 1599, 737, 7018, 3774, 9843, 2296, 2287, 9875, 2349, 2469, 8941, 4973, 3798, 54, 2938, 4665, 3942, 3951, 9400, 3094, 2248, 3376, 1926, 5180, 1773, 3681, 1808, 350, 6669, 826, 539, 5313, 6193, 5752, 9370, 2782, 8399, 4881, 3166, 4906, 5829, 4827, 29, 6899, 9012, 6986, 4175, 1035, 8320, 7802, 3777, 6340, 7798, 7705]\n",
    "\n",
    "        for i in range(n_ensemble):\n",
    "            seed = seed_list[i]\n",
    "\n",
    "            print('Initializing seed = ' + str(seed_list.index(seed)))\n",
    "            random.seed(seed)\n",
    "\n",
    "            # the pool of candidates to be examined\n",
    "            indices_to_explore = list(np.arange(len(self.X))).copy()\n",
    "            # the list of candidates we have already observed (add initial experiments)\n",
    "            observed_indices = random.sample(indices_to_explore, n_initial)\n",
    "\n",
    "            X_, y_, TopCount_ = [], [], []\n",
    "            top_indices_count = 0\n",
    "\n",
    "            # check if initial experiments contained top candidates\n",
    "            for i in observed_indices:\n",
    "                X_.append(self.X[i])\n",
    "                y_.append(self.y[i])\n",
    "\n",
    "                # if so increment top_indices count\n",
    "                if i in self.top_indices:\n",
    "                    top_indices_count += 1\n",
    "\n",
    "                # remove said indices from the list of indices to be explored\n",
    "                TopCount_.append(top_indices_count)\n",
    "                indices_to_explore.remove(i)\n",
    "\n",
    "            # this for loop ends when all candidates in pool are observed\n",
    "            for i in np.arange(len(indices_to_explore)):\n",
    "                # as AutoAM is a maximization optimization problem y_best will be the maximum y value examined so far\n",
    "                y_best = np.max(y_)\n",
    "\n",
    "                s_scaler = StandardScaler()\n",
    "                X_train = s_scaler.fit_transform(X_)\n",
    "                y_train = s_scaler.fit_transform([[i] for i in y_])\n",
    "\n",
    "                # select Surrogate Model for BO\n",
    "                if surrogate_model_name == \"RF\":\n",
    "                    n_est = kwargs.get(\"n_est\", 100)\n",
    "                    model = RF_Surrogate_Model(n_est, n_jobs=-1)\n",
    "                    model.fit(X_train, y_train)\n",
    "\n",
    "                elif surrogate_model_name == \"GP\":\n",
    "                    kernel_name = kwargs.get(\"kernel_name\", \"Mat52\")\n",
    "                    ARD_ = kwargs.get(\"ARD\", True)\n",
    "\n",
    "                    try:\n",
    "                        model = GP_Surrogate_Model(self.X, kernel_name, ARD_)\n",
    "                        model.fit(X_train, y_train)\n",
    "\n",
    "                    except:\n",
    "                        break\n",
    "\n",
    "                # calculate the next index to explore according to the acquisition function\n",
    "                next_index = self.find_next_index(indices_to_explore, model, acqusition_function_name, y_best)\n",
    "\n",
    "                X_.append(self.X[next_index])\n",
    "                y_.append(self.y[next_index])\n",
    "\n",
    "                # if next_index corresponds to a top candidate increment top candidates count\n",
    "                if next_index in self.top_indices:\n",
    "                    top_indices_count += 1\n",
    "\n",
    "                # Append top_indices_count, shows the number of top candidates found each ensemble\n",
    "                TopCount_.append(top_indices_count)\n",
    "\n",
    "                indices_to_explore.remove(next_index)\n",
    "                observed_indices.append(next_index)\n",
    "\n",
    "            try:\n",
    "                assert len(observed_indices) == len(self.X)\n",
    "\n",
    "                self.index_collection.append(observed_indices)\n",
    "                self.X_collection.append(X_)\n",
    "                self.y_collection.append(y_)\n",
    "                self.TopCount_collection.append(TopCount_)\n",
    "\n",
    "            except:\n",
    "                # Occurs in some cases of BO with GP, randomly code fails the above assertion\n",
    "                print('Error during train, moving on to next seed')\n",
    "\n",
    "            print('Finished seed')\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        master = np.array([self.index_collection, self.X_collection, self.y_collection, self.TopCount_collection, total_time])\n",
    "\n",
    "        # TODO: Name output file\n",
    "        np.save(surrogate_model_name + \"_final_run\", master)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Bayesian Optimization Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing seed = 0\n",
      "Finished seed\n",
      "Initializing seed = 1\n",
      "Finished seed\n",
      "Initializing seed = 2\n",
      "Finished seed\n",
      "Initializing seed = 3\n",
      "Finished seed\n",
      "Initializing seed = 4\n",
      "Finished seed\n",
      "Initializing seed = 5\n",
      "Finished seed\n",
      "Initializing seed = 6\n",
      "Finished seed\n",
      "Initializing seed = 7\n",
      "Finished seed\n",
      "Initializing seed = 8\n",
      "Finished seed\n",
      "Initializing seed = 9\n",
      "Finished seed\n"
     ]
    }
   ],
   "source": [
    "# load a dataset\n",
    "# dataset names = ['Crossed barrel', 'Perovskite', 'AgNP', 'P3HT', 'AutoAM']\n",
    "data_handler_object = Data_Handler('AutoAM')\n",
    "\n",
    "top_indices = data_handler_object.get_top_n_indices(0.05)\n",
    "X, y = data_handler_object.X_y_split()\n",
    "\n",
    "bayesian_optimization = BO(X, y, top_indices)\n",
    "\n",
    "# to run the code with Random Forests (roughly 1 hour runtime at 50 ensembles)\n",
    "bayesian_optimization.run_bayesian_optimization(n_ensemble = 10, \n",
    "                                                n_initial = 2, \n",
    "                                                surrogate_model_name = \"RF\",\n",
    "                                                acqusition_function_name = \"LCB\", \n",
    "                                                n_est = 100)\n",
    "\n",
    "# to run the code with Gaussian Process with ARD (roughly 2.5 hours runtime at 50 ensembles)\n",
    "# bayesian_optimization.run_bayesian_optimization(n_ensemble = 10, \n",
    "#                                                 n_initial = 2, \n",
    "#                                                 surrogate_model_name = \"GP\", \n",
    "#                                                 acqusition_function_name = \"LCB\", \n",
    "#                                                 kernel_name = \"Mat52\",\n",
    "#                                                 ARD = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
